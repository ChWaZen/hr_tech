{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 安裝及載入套件\n",
        "\n",
        "一旦安裝了套件，你需要在 Python 程序中導入它，以便使用該套件提供的功能。\n",
        "\n",
        "* pandas（pd）：數據分析和處理的強大工具，特別擅長處理表格數據。\n",
        "\n",
        "* numpy（np）：用於科學計算的庫，尤其擅長數組和矩陣操作。\n",
        "\n",
        "* csv：用於讀取和寫入 CSV 文件，即逗號分隔的值文件。\n",
        "\n",
        "* pytesseract：OCR 工具，將圖像中的文本轉換為可編輯的文本格式。\n",
        "\n",
        "* PIL (Image)：Python Imaging Library，用於圖像處理。\n",
        "\n",
        "* io（BytesIO）：允許操作內存中的二進制數據流，如讀寫二進制文件。\n",
        "\n",
        "* fitz (PyMuPDF)：另一種用於讀取和處理 PDF 和其他文檔的庫。\n",
        "\n",
        "* ast：用於處理 Python 的抽象語法樹，可以用來分析和修改 Python 代碼。\n",
        "\n",
        "* re：提供正則表達式工具，用於字符串的搜索和替換。\n",
        "\n",
        "* pdfplumber：提供從 PDF 文件中提取文本和表格的功能。\n",
        "\n",
        "* PyPDF2：用於讀取和處理 PDF 文件的庫。\n",
        "\n",
        "* tabula（read_pdf）：專門用於從 PDF 文件中提取表格數據。\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QXAjwhNa4upL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pdfplumber\n",
        "!pip install opencc-python-reimplemented\n",
        "!pip install tabula-py\n",
        "!pip install PyPDF2\n",
        "!pip install fitz\n",
        "!pip install pytesseract\n",
        "!pip install frontend\n",
        "!pip install pymupdf Pillow pytesseract\n",
        "!sudo apt-get install tesseract-ocr tesseract-ocr-chi-tra\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import io\n",
        "import fitz  # PyMuPDF\n",
        "import ast\n",
        "import re\n",
        "import pdfplumber\n",
        "import PyPDF2\n",
        "from tabula import read_pdf\n"
      ],
      "metadata": {
        "id": "WmSXyiHnD4Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OCR範例\n",
        "\n",
        "*   首先定義一個函數 extract_text_from_pdf_with_ocr\n",
        "*   函數接收一個參數：pdf檔案\n",
        "*   它使用 PyMuPDF（fitz）來打開和讀取 PDF 文件的每一頁，並將這些頁面轉換成圖像。\n",
        "*   利用 pytesseract 執行 OCR 處理，以識別和轉換圖像中的文字，最終將所有頁面的文字合併成一個字符串並返回。"
      ],
      "metadata": {
        "id": "ha4bo-O85PCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf_with_ocr(pdf_path, lang='chi_tra'):\n",
        "     doc = fitz.open(pdf_path)\n",
        "     full_text = \"\"\n",
        "\n",
        "     for page in doc:\n",
        "         # 將 PDF 頁面轉換為圖片\n",
        "         pix = page.get_pixmap()\n",
        "         img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "\n",
        "         # 使用 pytesseract 進行 OCR\n",
        "         page_text = pytesseract.image_to_string(img, lang=lang) # 指定使用繁體中文進行OCR\n",
        "         full_text += page_text + \"\\n\" # 新增每頁的文本，並在頁之間新增換行符\n",
        "\n",
        "     return full_text\n",
        "\n",
        "# 指定 PDF 檔案路徑\n",
        "pdf_path = 'A123456789.pdf'\n",
        "extracted_text = extract_text_from_pdf_with_ocr(pdf_path)\n",
        "\n",
        "print(extracted_text)"
      ],
      "metadata": {
        "id": "ToqvMB-L5RSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 提取自傳\n",
        "\n",
        "*   首先定義了一個函數 extract_autobio，用於從 PDF 檔案中提取特定範圍的文字。\n",
        "*   函數接收三個參數：PDF 檔案名稱 pdf_file_name、起始關鍵字 keyword 和結束關鍵字 keyword2。\n",
        "*   函數使用 pdfplumber 打開 PDF 文件，遍歷每一頁並提取文字。\n",
        "\n",
        "*   當在頁面文字中找到起始關鍵字 keyword 後，尋找結束關鍵字 keyword2。 若找到，則從 keyword2 之後到下一個句號之間的文字被提取出來，這部分文本通常包含了自傳的主要內容。"
      ],
      "metadata": {
        "id": "Dy_J-lPW6H2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_autobio(pdf_file_name, keyword, keyword2):\n",
        "  with pdfplumber.open(pdf_file_name) as pdf:\n",
        "      for page in pdf.pages:\n",
        "          page_text = page.extract_text()\n",
        "          if keyword in page_text:\n",
        "              index_keyword = page_text.find(keyword)\n",
        "              index_keyword2 = page_text.find(keyword2, index_keyword)\n",
        "              if index_keyword2 != -1:\n",
        "                  extracted_text = page_text[index_keyword2 + len(keyword2):]\n",
        "                  last_period_index = extracted_text.rfind(\"。\")\n",
        "                  if last_period_index != -1:\n",
        "                      extracted_text = extracted_text[:last_period_index+1]\n",
        "                  return extracted_text.strip()\n",
        "\n",
        "keyword = \"自傳內容\"\n",
        "keyword2 = \"---------------------------------------------------------------------\""
      ],
      "metadata": {
        "id": "Mumph9SGFdKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_autobio('A123456789.pdf', keyword, keyword2)"
      ],
      "metadata": {
        "id": "PcMzzjVmV00i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_autobio('B123456789.pdf', keyword, keyword2)"
      ],
      "metadata": {
        "id": "LvxiAlIdWydb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 提取履歷表格\n",
        "\n",
        "\n",
        "*   首先定義了一個函數 extract_table，用於從 PDF 文件中提取所有表格並將它們合併成一個單一的 pandas DataFrame。\n",
        "*   函數使用 tabula 的 read_pdf 函數讀取指定 PDF 文件中的表格，設置 lattice=True 以精確識別表格線條，並允許提取多個表格。\n",
        "*   然後，使用 pd.concat 將提取的所有表格（DataFrame 列表）合併為一個 DataFrame，並重置索引以避免重複。\n",
        "\n"
      ],
      "metadata": {
        "id": "URmtRYbx6K0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_table(pdf_file_name):\n",
        "  # Try to read tables from the PDF\n",
        "  dfs = read_pdf(pdf_file_name, lattice=True, multiple_tables=True)\n",
        "  concatenated_df = pd.concat(dfs, ignore_index=True)\n",
        "  return concatenated_df"
      ],
      "metadata": {
        "id": "Sd4xvZjg0BTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_table('A123456789.pdf')"
      ],
      "metadata": {
        "id": "Ma1nhpj5V2ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_table('B123456789.pdf')"
      ],
      "metadata": {
        "id": "5KMHqo6PWYm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 整理欄位的函數\n",
        "\n",
        "*  clean_items 函數：這個函數接收一個列表 items，並對列表中的每個字符串元素進行清理，移除空格、換行符、車輛回退符以及特定子串（如\"(最高)\"和\"(次高)\"）。此外，如果條目包含\"專業證照\"，則不會對該條目進行清理。最後，所有空字符串被替換為 None。\n",
        "\n",
        "*   extract_and_clean_data 函數：此函數用於從給定的數據流（如迭代器 resume_tuples）中提取包含特定關鍵詞之間的數據部分。它首先尋找 keyword_1 標誌開始的點，然後從下一個元素開始收集數據，直到遇到 keyword_2，標誌數據收集的結束。收集的數據經過 clean_items 函數清理，並確保最終數據長度符合預期（通過添加空字符串調整）。最終，函數返回清理和格式化後的數據列表。"
      ],
      "metadata": {
        "id": "PsxSdEHW5weF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_items(items):\n",
        "    clean_items = [item.replace(' ', '').replace('\\r', '').replace('\\n', '').replace('(最高)', '').replace('(次高)', '') for item in items if isinstance(item, str) and '專業證照' not in item]\n",
        "    clean_items = [x if x != \"\" else None for x in clean_items]\n",
        "    return clean_items\n",
        "\n",
        "def extract_and_clean_data(data, resume_row, keyword_1, keyword_2, length):\n",
        "    data_section = []\n",
        "    while keyword_1 not in str(resume_row):\n",
        "        resume_row = next(resume_tuples)\n",
        "    resume_row = next(resume_tuples)\n",
        "\n",
        "    while keyword_2 not in str(resume_row):\n",
        "        data_section.append(resume_row)\n",
        "        resume_row = next(resume_tuples)\n",
        "\n",
        "    for s in data_section:\n",
        "        if isinstance(s[1], str):\n",
        "            data.extend(clean_items(s))\n",
        "\n",
        "    data.extend([''] * (length - len(data)))\n",
        "    return data, resume_row"
      ],
      "metadata": {
        "id": "Ig1FPMO3P9OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 呈現整理好的欄位內容"
      ],
      "metadata": {
        "id": "wz2yBOJS5-TU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resume_df = extract_table('A123456789.pdf')\n",
        "resume_tuples = resume_df.itertuples(index=False, name=None)\n",
        "\n",
        "data = ['A123456789.pdf']\n",
        "licience = []\n",
        "\n",
        "resume_row = None\n",
        "data, resume_row = extract_and_clean_data(data, resume_row, '學歷背景', '工作經驗', 10)\n",
        "print(data)"
      ],
      "metadata": {
        "id": "VTmRIPsKlbcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, resume_row = extract_and_clean_data(data, resume_row, '工作經驗', '外語能力', 26)\n",
        "print(data)"
      ],
      "metadata": {
        "id": "mPmaDmWg2NpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.append(resume_row[1]) # 外語能力\n",
        "print(data)"
      ],
      "metadata": {
        "id": "ac6Jvm_t23F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  while '證照' not in str(resume_row):\n",
        "      resume_row = next(resume_tuples)\n",
        "\n",
        "  licience = []\n",
        "\n",
        "  while True:\n",
        "      licience.extend(resume_row)\n",
        "      resume_row = next(resume_tuples)\n",
        "\n",
        "except StopIteration:\n",
        "    pass\n",
        "\n",
        "print(licience)\n",
        "print(clean_items(licience))"
      ],
      "metadata": {
        "id": "pxqgc_Yu25HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  data.append(clean_items(licience))\n",
        "  data.append(extract_autobio(name, keyword, keyword2))\n",
        "\n",
        "  print(data)"
      ],
      "metadata": {
        "id": "zHX16AAB3qo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 對所有資料進行整理\n",
        "\n",
        "\n",
        "\n",
        "*   這段程式碼從一系列 PDF 文件中提取特定的履歷信息，並將這些信息整合成一個統一的數據結構，最後將數據輸出到一個 CSV 文件中。\n",
        "*   首先從每個 PDF 中提取表格數據，然後使用自定義函數依次提取學歷背景、工作經驗和外語能力等部分。還從每個文件中提取證照信息和自傳內容。\n",
        "*   每個 PDF 的提取結果存儲為列表形式，然後所有列表被彙總到一個大列表中。最終，這個大列表被轉換為一個 pandas DataFrame，並存儲到 CSV 文件中。\n",
        "\n"
      ],
      "metadata": {
        "id": "jiBiFvGP9zRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################ 撈出資料 ######################\n",
        "all_data = []\n",
        "\n",
        "for name in ['A123456789.pdf', 'B123456789.pdf']:\n",
        "  resume_df = extract_table(name)\n",
        "  resume_tuples = resume_df.itertuples(index=False, name=None)\n",
        "\n",
        "  data = [name]\n",
        "  licience = []\n",
        "  try:\n",
        "      resume_row = None\n",
        "      data, resume_row = extract_and_clean_data(data, resume_row, '學歷背景', '工作經驗', 10)\n",
        "      data, resume_row = extract_and_clean_data(data, resume_row, '工作經驗', '外語能力', 26)\n",
        "\n",
        "      data.append(resume_row[1]) # 外語能力\n",
        "\n",
        "      while '證照' not in str(resume_row):\n",
        "          resume_row = next(resume_tuples)\n",
        "\n",
        "      licience = []\n",
        "\n",
        "      while True:\n",
        "          licience.extend(resume_row)\n",
        "          resume_row = next(resume_tuples)\n",
        "\n",
        "\n",
        "  except StopIteration:\n",
        "      pass\n",
        "  data.append(clean_items(licience))\n",
        "  data.append(extract_autobio(name, keyword, keyword2))\n",
        "  all_data.append(data)\n",
        "\n",
        "# Convert the list of lists to a DataFrame\n",
        "result_df = pd.DataFrame(all_data)\n",
        "result_df.to_csv('output.csv', index=False)"
      ],
      "metadata": {
        "id": "oK1RQXafQ0al"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fbp_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}